# Chat-with-PDF-using-Groq-LLM
ğŸ“ Description
This application enables natural language interaction with the contents of uploaded PDF documents. Once the PDFs are uploaded, the text is extracted, chunked, and embedded using sentence-transformers. The system then stores these embeddings in a Chroma vector database and retrieves the most relevant chunks to answer user queries using LangChainâ€™s QA chain and Groqâ€™s LLM.

It supports:

Multi-page and multi-document upload

Free and local embedding generation (no Hugging Face token needed)

Accurate, grounded answers with reasoning

Clean and simple UI using Streamlit

ğŸš€ Features
ğŸ“š Upload multiple PDF documents

ğŸ” Ask questions and get answers from the content

ğŸ¤– Uses sentence-transformers/all-MiniLM-L6-v2 for embeddings

ğŸ§  Powered by Groq's Gemma2-9b-It model (switchable to LLaMA3)

ğŸ’¾ Embeddings stored in ChromaDB for semantic search

ğŸ’¬ Streamlit UI for easy user interaction

ğŸ” .env file support for secure API key management

ğŸ“ Project Structure:
```.
â”œâ”€â”€ app.py                 # Main Streamlit application
â”œâ”€â”€ chromadb_index/        # Stores Chroma vector index
â”œâ”€â”€ .env                   # Contains API keys
â”œâ”€â”€ README.md              # Project documentation
 ```
ğŸ”‘ Environment Setup
Create a .env file in your root folder and add
```GROQ_API_KEY=your_groq_api_key_here```

Install dependencies:
```pip install -r requirements.txt```

â–¶ï¸ Run the App
```streamlit run app.py```
ğŸ’¡ How It Works
PDF Upload: Upload one or more PDF files via the sidebar.

Text Extraction: Extracts all text from uploaded PDFs.

Chunking: Splits text into manageable chunks using RecursiveCharacterTextSplitter.

Embedding: Generates vector embeddings locally using MiniLM.

Storage: Embeddings are stored in ChromaDB (chromadb_index/).

Querying: User enters a question, relevant chunks are retrieved.

LLM Response: Groq LLM answers based on retrieved content using a custom prompt.



